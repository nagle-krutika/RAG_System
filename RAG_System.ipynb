{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c76bcd-204d-4873-bce8-94a8e06d42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d502dab3-32bc-4e8c-bf1f-be3d54619c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b2a9d3-4561-4523-99cc-455df7cbc647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading document\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"LNCB_Paper.pdf\")\n",
    "data = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0acc308-d638-4ed0-aad0-ab292dd28e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 568, which is longer than the specified 500\n",
      "Created a chunk of size 506, which is longer than the specified 500\n",
      "Created a chunk of size 633, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "# splitting documents into chunks\n",
    "\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size = 500, chunk_overlap = 100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f4f89fe-2e28-4cba-9db6-1c879476a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading googleGenAI embeddings\n",
    "\n",
    "# reading google api key\n",
    "f = open(r\"C:\\Users\\nishk\\Desktop\\Innomatics\\LangChain_RAG\\Keys\\api_key.txt\")\n",
    "google_api_key = f.read()\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key = google_api_key,\n",
    "                                     model = \"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af9b642f-2925-41c2-a853-17ae9691df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating chunks embeddings\n",
    "# store the chunks in vector store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# embedd each chunk and load it into the vector store\n",
    "database = Chroma.from_documents(chunks, embedding_model, persist_directory = \"Chroma_db\")\n",
    "\n",
    "database.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6481f13-fa2d-469e-aa51-9ad54f616b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting connection with chromadb\n",
    "\n",
    "db_connection = Chroma(persist_directory = \"Chroma_db\", embedding_function = embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab75981-334e-4a65-88c1-b89d6a593c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Chroma db_connection to retriever object\n",
    "\n",
    "retriever = db_connection.as_retriever(search_kwargs = {\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1ece0a-0f20-4c9e-a187-4b0da8763f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating chat template\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"You are a Helpful AI Bot. \n",
    "    You take the context and question from user. Your answer should be based on the specific context.\"\"\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Aswer the question based on the given context.\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: \n",
    "    {question}\n",
    "    \n",
    "    Answer: \"\"\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fedce1f6-09eb-468f-b71f-697ba6b159ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating chat model\n",
    "chat_model = ChatGoogleGenerativeAI(google_api_key = google_api_key, \n",
    "                                    model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a3c585a-765e-43e2-bb84-721fcc3e1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating output parser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0770900e-ccce-46ce-9a11-5a0871d3c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating chain\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | chat_template\n",
    "    | chat_model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "661a2cbe-e452-4b35-bdbc-8c782c50f8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Short Summary of the Paper:\n",
       "\n",
       "This paper explores the challenge of balancing efficiency and quality when applying compressive memory techniques to large language models (LLMs). While recent efforts have focused on system-level optimizations for attention mechanisms, there's still a need for simpler and more effective memory compression methods. \n",
       "\n",
       "The authors propose a novel approach that leverages linear attention mechanisms for memory update and retrieval processes. This method draws inspiration from Katharopoulos et al. (2020) due to its simplicity and performance. The paper also highlights the use of a 32K input length during fine-tuning and a 500K input length for evaluation, along with specific decoding parameters for generating summaries. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as markdown\n",
    "\n",
    "response = rag_chain.invoke(\"give me the short summary of the paper\")\n",
    "markdown(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
